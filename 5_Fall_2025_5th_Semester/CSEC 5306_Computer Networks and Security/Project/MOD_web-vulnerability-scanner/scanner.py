import argparse
import urllib.request
from urllib.error import URLError, HTTPError
from collections import defaultdict
from colorama import Fore, Style
import contextlib
import sys
from io import StringIO

from VulnerabilityScanner.CommonPortsCheck import CommonPortsCheck
from VulnerabilityScanner.Crawler import Crawler
from VulnerabilityScanner.LFIScanner import LfiScanner
from VulnerabilityScanner.ReportGenerator import ReportGenerator
from VulnerabilityScanner.SecurityHeaders import SecurityHeaders
from VulnerabilityScanner.SqlInjection import SqlInjection
from VulnerabilityScanner.XssScanner import XssScanner

def perform_scans(quiet, givenurl, urls, xsspayload, lfi_url, nohttps, summary):
    global_vulnerabilities = set()
    vuln_counter = defaultdict(int)
    report = ReportGenerator(givenurl)

    output_file = f"{givenurl.replace('https://', '').replace('http://', '').replace('/', '_')}_scan_results.txt"
    with open(output_file, 'w') as file:
        ports = CommonPortsCheck(quiet)
        ports.scan_ports(givenurl, report)

        secheaders = SecurityHeaders(givenurl, quiet, nohttps)
        secheaders.check_security_headers(report)

        if lfi_url:
            lfi_advanced = LfiScanner(quiet, lfi_url)
            lfi_advanced.advanced_lfi(report)

        if urls:
            file.write("\n[Info] Crawled URLs:\n" + "\n".join(urls) + "\n")
            report.write_to_report("\n[Info] Crawled URLs:\n" + "\n".join(urls) + "\n")

            form_count = 0
            for url in urls:
                forms = Crawler.get_forms(url, quiet)
                form_count += len(forms)
                file.write(f"\n[Info] Found {len(forms)} forms on {url}\n")
                report.write_to_report(f"\n[Info] Found {len(forms)} forms on {url}\n")

            file.write(f"\n[Summary] Total Forms Found: {form_count}\n")
            report.write_to_report(f"\n[Summary] Total Forms Found: {form_count}\n")

        if urls:
            for url in urls:
                file.write(f"\n[*****]  Checking URL: {url}  [*****] \n")
                report.write_to_report(f"\n [*****]  Checking URL: {url}  [*****] \n")
                if not quiet:
                    print(f"\n[*****]  Checking URL: {url}  [*****] \n")
                attacks(report, url, quiet, xsspayload, global_vulnerabilities, vuln_counter)
        else:
            attacks(report, givenurl, quiet, xsspayload, global_vulnerabilities, vuln_counter)

        if summary:
            summary_text = '\n[Summary] Vulnerability Counts:\n'
            file.write(summary_text)
            report.write_to_report(summary_text)
            if not quiet:
                print(Fore.CYAN + summary_text + Style.RESET_ALL)
            for vuln_type, count in vuln_counter.items():
                vuln_summary = f"\n{vuln_type}: {count}\n"
                file.write(vuln_summary)
                report.write_to_report(vuln_summary)
                if not quiet:
                    print(vuln_summary)

       

    if not quiet:
        print(f"Scan results saved to {output_file}")

def check_lfi_url(url, quiet=False):
    if 'file=' not in url:
        if not quiet:
            print(Fore.YELLOW + '[!] LFI URL may not be vulnerable (missing "file=" parameter)' + Style.RESET_ALL)
    try:
        if '=' in url and '?' in url:
            request_website = urllib.request.urlopen(url).getcode()
            if request_website == 200:
                return True
    except (HTTPError, URLError) as e:
        if not quiet:
            print(Fore.RED + f"[!] Error checking LFI URL: {e}" + Style.RESET_ALL)
    return False

def attacks(report, url, quiet, xsspayload, vulnerability_tracker, vuln_counter):
    xss_scan = XssScanner(quiet, url, vulnerability_tracker)
    xss_scan.scan_host(report, xsspayload)
    vuln_counter['XSS'] += xss_scan.get_vuln_count()

    lfi_scan = LfiScanner(quiet, url)
    lfi_scan.basic_scan_host(report)
    vuln_counter['LFI'] += lfi_scan.get_vuln_count()

    sql_inj = SqlInjection(quiet, url)
    sql_inj.scan_host(report)
    vuln_counter['SQLi'] += sql_inj.get_vuln_count()

def main():
    xsspayload = ''
    lfi_url = ''
    urls = []

    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--crawler', help='Crawl to provided depth', dest='crawler', type=int, default=1)
    parser.add_argument('-q', '--quiet', help='quiet mode - no console output generated', dest='quiet', action='store_true')
    parser.add_argument('-u', '--url', help='Provide URL for web application, example: https://www.example.com', required=True, dest='url')
    parser.add_argument('-xp', '--xsspayload', help='Path to XSS payload', dest='xsspayload')
    parser.add_argument('-lu', '--lfiurl', help='Provide url with parameters to perform advanced LFI scan, example: https://www.example.com/page.php?file=', dest='lfi_url')
    parser.add_argument('-nh', '--nohttps', help='Don\'t check for HTTPS header', dest='nohttps', action='store_true')
    parser.add_argument('-s', '--summary', help='Show vulnerability summary at end', action='store_true')
    arguments = parser.parse_args()

    if arguments.quiet:
        sys.stdout = StringIO()
        sys.stderr = StringIO()

    try:
        check_site = urllib.request.urlopen(arguments.url)
        if not arguments.quiet:
            print("Site responded with code " + str(check_site.getcode()))
    except (HTTPError, URLError) as e:
        print("Connection to: " + arguments.url + " could not be established, error code: " + str(e))
        exit()

    if arguments.url.endswith('/'):
        arguments.url = arguments.url[:-1]

    if not arguments.quiet:
        print(arguments.url)

    if arguments.xsspayload:
        xsspayload = arguments.xsspayload

    if arguments.lfi_url:
        check_url = check_lfi_url(arguments.lfi_url, arguments.quiet)
        if check_url:
            lfi_url = arguments.lfi_url
        else:
            if not arguments.quiet:
                print("LFI URL doesn't respond, performing scan without advanced lfi")

    if arguments.crawler:
        urls = Crawler.deep_crawl(arguments.url, arguments.crawler)
        if arguments.url not in urls:
            urls.append(arguments.url)
            
    


    perform_scans(arguments.quiet, arguments.url, urls, xsspayload, lfi_url, arguments.nohttps, arguments.summary)

if __name__ == '__main__':
    main()

import argparse
import urllib.request
from urllib.error import URLError, HTTPError
from collections import defaultdict
from colorama import Fore, Style

from VulnerabilityScanner.CommonPortsCheck import CommonPortsCheck
from VulnerabilityScanner.Crawler import Crawler
from VulnerabilityScanner.LFIScanner import LfiScanner
from VulnerabilityScanner.ReportGenerator import ReportGenerator
from VulnerabilityScanner.SecurityHeaders import SecurityHeaders
from VulnerabilityScanner.SqlInjection import SqlInjection
from VulnerabilityScanner.XssScanner import XssScanner


def perform_scans(quiet, givenurl, urls, xsspayload, lfi_url, nohttps, summary):
    """
    Function performing scans and executing attacks

    Enhancements:
    - Uses a vulnerability counter to track each vuln type (XSS, LFI, SQLi)
    - Logs crawled URLs into report
    - Adds final summary to report and optionally prints to console
    - Outputs results to a .txt file
    """
    global_vulnerabilities = set()
    vuln_counter = defaultdict(int)
    report = ReportGenerator(givenurl)

    # Open a .txt file to save the results
    output_file = f"{givenurl.replace('https://', '').replace('http://', '').replace('/', '_')}_scan_results.txt"
    with open(output_file, 'w') as file:
        ports = CommonPortsCheck(quiet)
        ports.scan_ports(givenurl, report)

        secheaders = SecurityHeaders(givenurl, quiet, nohttps)
        secheaders.check_security_headers(report)

        if lfi_url:
            lfi_advanced = LfiScanner(quiet, lfi_url)
            lfi_advanced.advanced_lfi(report)

        if urls:
            file.write("\n[Info] Crawled URLs:\n" + "\n".join(urls) + "\n")
            report.write_to_report("\n[Info] Crawled URLs:\n" + "\n".join(urls) + "\n")

            form_count = 0
            for url in urls:
                forms = Crawler.get_forms(url)  # Assuming `get_forms` is a method in `Crawler`
                form_count += len(forms)
                file.write(f"\n[Info] Found {len(forms)} forms on {url}\n")
                report.write_to_report(f"\n[Info] Found {len(forms)} forms on {url}\n")

            file.write(f"\n[Summary] Total Forms Found: {form_count}\n")
            report.write_to_report(f"\n[Summary] Total Forms Found: {form_count}\n")

        if urls:
            for url in urls:
                file.write(f"\n[*****]  Checking URL: {url}  [*****] \n")
                report.write_to_report(f"\n [*****]  Checking URL: {url}  [*****] \n")
                if not quiet:
                    print(f"\n[*****]  Checking URL: {url}  [*****] \n")
                attacks(report, url, quiet, xsspayload, global_vulnerabilities, vuln_counter)
        else:
            attacks(report, givenurl, quiet, xsspayload, global_vulnerabilities, vuln_counter)

        if summary:
            summary_text = '\n[Summary] Vulnerability Counts:\n'
            file.write(summary_text)
            print(Fore.CYAN + summary_text + Style.RESET_ALL)
            for vuln_type, count in vuln_counter.items():
                vuln_summary = f"\n{vuln_type}: {count}\n"
                file.write(vuln_summary)
                print(vuln_summary)
                report.write_to_report(vuln_summary)

    print(f"Scan results saved to {output_file}")


def check_lfi_url(url):
    if 'file=' not in url:
        print(Fore.YELLOW + '[!] LFI URL may not be vulnerable (missing "file=" parameter)' + Style.RESET_ALL)
    """
    Check if provided URL has specific LFI-related characters
    """
    if '=' in url and '?' in url:
        request_website = urllib.request.urlopen(url).getcode()
        if request_website == 200:
            return True
        else:
            return False


def attacks(report, url, quiet, xsspayload, vulnerability_tracker, vuln_counter):
    """
    Execute XSS, LFI, SQLi attacks for a given URL
    Tracks and counts found vulnerabilities
    """
    xss_scan = XssScanner(quiet, url, vulnerability_tracker)
    xss_scan.scan_host(report, xsspayload)
    vuln_counter['XSS'] += len(xss_scan.vulnerabilities_tracker)

    lfi_scan = LfiScanner(quiet, url)
    lfi_scan.basic_scan_host(report)
    # vuln_counter['LFI'] += 1  # Approximate, assumes detection happens during scan
    vuln_counter['LFI'] += len(lfi_scan.vulnerabilities_tracker)  # Update to track vulnerabilities

    sql_inj = SqlInjection(quiet, url)
    sql_inj.scan_host(report)
    # vuln_counter['SQLi'] += 1  # Same approximation as above
    vuln_counter['SQLi'] += len(sql_inj.vulnerabilities_tracker)  # Update to track vulnerabilities

def main():
    """
    Parse arguments, validate input URL, and launch scans
    """
    xsspayload = ''
    lfi_url = ''
    urls = []

    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--crawler', help='Crawl to provided depth', dest='crawler', type=int, default=1)
    parser.add_argument('-q', '--quiet', help='quiet mode - no console output generated', dest='quiet', action='store_true')
    parser.add_argument('-u', '--url', help='Provide URL for web application, example: https://www.example.com', required=True, dest='url')
    parser.add_argument('-xp', '--xsspayload', help='Path to XSS payload', dest='xsspayload')
    parser.add_argument('-lu', '--lfiurl', help='Provide url with parameters to perform advanced LFI scan, example: https://www.example.com/page.php?file=', dest='lfi_url')
    parser.add_argument('-nh', '--nohttps', help='Don\'t check for HTTPS header', dest='nohttps', action='store_true')
    parser.add_argument('-s', '--summary', help='Show vulnerability summary at end', action='store_true')
    arguments = parser.parse_args()

    try:
        check_site = urllib.request.urlopen(arguments.url)
        print("Site responded with code " + str(check_site.getcode()))
    except (HTTPError, URLError) as e:
        print("Connection to: " + arguments.url + " could not be established, error code: " + str(e))
        exit()

    if arguments.url.endswith('/'):
        arguments.url = arguments.url[:-1]
    print(arguments.url)

    if arguments.xsspayload:
        xsspayload = arguments.xsspayload

    if arguments.lfi_url:
        check_url = check_lfi_url(arguments.lfi_url)
        if check_url:
            lfi_url = arguments.lfi_url
        else:
            print("LFI URL doesn't respond, performing scan without advanced lfi")

    if arguments.crawler:
        urls = Crawler.deep_crawl(arguments.url, arguments.crawler)
        if arguments.url not in urls:
            urls.append(arguments.url)

    perform_scans(arguments.quiet, arguments.url, urls, xsspayload, lfi_url, arguments.nohttps, arguments.summary)


if __name__ == '__main__':
    main()